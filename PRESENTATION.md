# ğŸš€ OMNIVAULT
## AI-Powered Intelligent Chat Assistant with RAG Technology

---

# ğŸ“‹ Table of Contents

1. [Project Overview](#-project-overview)
2. [Key Features](#-key-features)
3. [Technology Stack](#-technology-stack)
4. [System Architecture](#-system-architecture)
5. [How It Works](#-how-it-works)
6. [Chat Modes](#-chat-modes)
7. [RAG Pipeline](#-rag-pipeline)
8. [User Interface](#-user-interface)
9. [Future Enhancements](#-future-enhancements)

---

# ğŸ¯ Project Overview

**OMNIVAULT** is a modern, AI-powered chat assistant that combines the power of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) technology to provide intelligent, context-aware responses.

### What Makes It Special?
- **Local AI Model**: Runs entirely on your machine - no cloud API costs
- **Document Intelligence**: Upload your documents and chat with them
- **Deep Research**: Web search integration for comprehensive research
- **Persistent Memory**: Chat history is saved and restored across sessions
- **Beautiful UI**: Modern, responsive interface with dark/light themes

---

# âœ¨ Key Features

## ğŸ’¬ Intelligent Chat
- Natural conversation with AI
- Context-aware responses using conversation history
- Support for follow-up questions

## ğŸ“„ Document Upload & Processing
- Upload PDFs, Word documents, text files, images, and code
- Automatic text extraction and indexing
- Asynchronous processing for large files

## ğŸ” RAG (Retrieval-Augmented Generation)
- Semantic search across uploaded documents
- ChromaDB vector database for efficient retrieval
- Context injection for accurate answers

## ğŸŒ Deep Research Mode
- Live web search integration (Tavily API)
- Combines web results with local documents
- Comprehensive research synthesis

## ğŸ“ Summarization
- Intelligent text summarization
- Works with uploaded documents
- Clean, structured output

## ğŸ’¾ Persistent Chat History
- All conversations saved automatically
- Resume conversations across sessions
- Sidebar with chat management

## ğŸ¨ Modern UI/UX
- Dark theme (Navy Blue) and Light theme (Beige)
- Responsive design for all screen sizes
- Code block rendering with syntax highlighting
- Copy button for code snippets

---

# ğŸ›  Technology Stack

## Frontend
| Technology | Purpose |
|------------|---------|
| **React.js** | UI Framework |
| **JavaScript (ES6+)** | Programming Language |
| **CSS3** | Styling & Animations |
| **Marked.js** | Markdown Rendering |
| **Highlight.js** | Code Syntax Highlighting |

## Backend
| Technology | Purpose |
|------------|---------|
| **Python 3.12** | Programming Language |
| **Flask** | Web Framework & REST API |
| **Flask-CORS** | Cross-Origin Resource Sharing |

## AI/ML Stack
| Technology | Purpose |
|------------|---------|
| **Llama 3.2 3B Instruct** | Large Language Model |
| **Hugging Face Transformers** | Model Loading & Inference |
| **PyTorch** | Deep Learning Framework |
| **BitsAndBytes** | 4-bit Quantization for GPU efficiency |

## RAG & Vector Database
| Technology | Purpose |
|------------|---------|
| **ChromaDB** | Vector Database |
| **Sentence-Transformers** | Text Embeddings (all-MiniLM-L6-v2) |
| **LangChain (concepts)** | Document Chunking |

## Document Processing
| Technology | Purpose |
|------------|---------|
| **PyPDF2** | PDF Extraction |
| **python-docx** | Word Document Processing |
| **Pillow + pytesseract** | Image OCR |

## Web Search
| Technology | Purpose |
|------------|---------|
| **Tavily API** | Real-time Web Search |

---

# ğŸ— System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OMNIVAULT ARCHITECTURE                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   React Frontend â”‚â—„â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   Flask Backend  â”‚
     â”‚    (Port 3000)   â”‚  HTTP   â”‚    (Port 5000)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                                â”‚                                â”‚
          â–¼                                â–¼                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Llama 3.2  â”‚              â”‚    ChromaDB      â”‚             â”‚   Tavily API    â”‚
   â”‚   3B Model   â”‚              â”‚  Vector Database â”‚             â”‚   Web Search    â”‚
   â”‚  (GPU/CPU)   â”‚              â”‚   (Embeddings)   â”‚             â”‚   (Optional)    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                                â”‚
          â”‚                                â”‚
          â–¼                                â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   Response   â”‚              â”‚    Document      â”‚
   â”‚  Generation  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚    Retrieval     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   Context    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# âš™ï¸ How It Works

## 1ï¸âƒ£ User Input
User types a message or uploads a document through the React frontend.

## 2ï¸âƒ£ Request Processing
The Flask backend receives the request and determines the mode (Chat, Summarize, Deep Research).

## 3ï¸âƒ£ Document Retrieval (RAG)
- User query is converted to embeddings
- ChromaDB performs semantic similarity search
- Top-K relevant document chunks are retrieved

## 4ï¸âƒ£ Context Augmentation
- Retrieved documents are added to the prompt
- Conversation history is included for context
- System prompts guide the model's behavior

## 5ï¸âƒ£ LLM Generation
- Llama 3.2 3B model generates the response
- Uses 4-bit quantization for memory efficiency
- Runs on GPU (CUDA) or CPU

## 6ï¸âƒ£ Response Formatting
- Response is post-processed for clean formatting
- Markdown is rendered in the frontend
- Code blocks get syntax highlighting

---

# ğŸ­ Chat Modes

## ğŸ’¬ Chat Mode
- **Purpose**: General conversation with document context
- **Temperature**: 0.7 (balanced creativity)
- **RAG Enabled**: Yes (Top 3 documents)
- **Max Tokens**: 1024

## ğŸ“ Summarize Mode
- **Purpose**: Condensed summaries of text/documents
- **Temperature**: 0.3 (focused, deterministic)
- **RAG Enabled**: No
- **Max Tokens**: 512

## ğŸ”¬ Deep Research Mode
- **Purpose**: Comprehensive research with web + documents
- **Temperature**: 0.5 (balanced)
- **RAG Enabled**: Yes (Top 10 documents)
- **Web Search**: Enabled
- **Max Tokens**: 2048

## ğŸ’» Coding Mode
- **Purpose**: Code assistance and generation
- **Temperature**: 0.2 (precise, minimal hallucination)
- **RAG Enabled**: Yes (Top 5 documents)
- **Max Tokens**: 1536

---

# ğŸ“š RAG Pipeline

## Document Ingestion Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Upload     â”‚â”€â”€â”€â”€â–ºâ”‚   Extract    â”‚â”€â”€â”€â”€â–ºâ”‚    Chunk     â”‚â”€â”€â”€â”€â–ºâ”‚   Embed &    â”‚
â”‚   Document   â”‚     â”‚    Text      â”‚     â”‚    Text      â”‚     â”‚    Store     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     PDF/DOCX            PyPDF2/           512 chars/chunk         ChromaDB
     TXT/Images          docx/OCR          50 char overlap       Vector Store
```

## Query Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    User      â”‚â”€â”€â”€â”€â–ºâ”‚   Generate   â”‚â”€â”€â”€â”€â–ºâ”‚   Vector     â”‚â”€â”€â”€â”€â–ºâ”‚  Retrieve    â”‚
â”‚    Query     â”‚     â”‚  Embeddings  â”‚     â”‚   Search     â”‚     â”‚  Top-K Docs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      MiniLM-L6-v2         Cosine Similarity     Relevant Chunks
```

## Embedding Model
- **Model**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimensions**: 384
- **Speed**: Fast inference
- **Quality**: High semantic understanding

---

# ğŸ–¥ User Interface

## Sidebar
- âœ… Chat history list
- âœ… New chat button
- âœ… Delete chat option
- âœ… Rename chat
- âœ… Settings access

## Chat Area
- âœ… Message bubbles (user & assistant)
- âœ… Markdown rendering
- âœ… Code blocks with copy button
- âœ… File upload with visual chips
- âœ… Mode selector dropdown
- âœ… Thinking indicator during generation

## Settings
- âœ… Theme toggle (Dark/Light)
- âœ… Privacy policy
- âœ… Terms of use

---

# ğŸš€ Future Enhancements

## Planned Features
- ğŸ”œ Streaming responses (real-time token display)
- ğŸ”œ Voice input/output support
- ğŸ”œ Multi-language support
- ğŸ”œ Custom model fine-tuning
- ğŸ”œ Plugin system for extensions
- ğŸ”œ Export conversations to PDF
- ğŸ”œ Collaborative chat sessions
- ğŸ”œ Advanced document management

## Performance Improvements
- ğŸ”œ Model caching optimization
- ğŸ”œ Batch processing for multiple documents
- ğŸ”œ Response caching for common queries

---

# ğŸ“Š Technical Specifications

| Specification | Value |
|---------------|-------|
| **LLM Model** | Llama 3.2 3B Instruct |
| **Quantization** | 4-bit (BitsAndBytes) |
| **Embedding Model** | all-MiniLM-L6-v2 |
| **Vector Database** | ChromaDB |
| **Chunk Size** | 512 characters |
| **Chunk Overlap** | 50 characters |
| **Max File Size** | 50 MB |
| **Supported Formats** | PDF, DOCX, TXT, MD, PNG, JPG, PY, JS, etc. |

---

# ğŸ“ Conclusion

**OMNIVAULT** demonstrates the power of combining:
- ğŸ¤– **Local LLMs** for privacy and cost-efficiency
- ğŸ“š **RAG Technology** for accurate, grounded responses
- ğŸŒ **Web Search** for up-to-date information
- ğŸ’¾ **Persistent Storage** for seamless user experience
- ğŸ¨ **Modern UI** for intuitive interaction

### The Future of AI Assistants is Local, Intelligent, and Context-Aware!

---

# ğŸ™ Thank You!

## Questions?

**Project**: OMNIVAULT  
**Technologies**: React, Flask, Llama 3.2, ChromaDB, RAG  
**GitHub**: [Your Repository Link]

---
